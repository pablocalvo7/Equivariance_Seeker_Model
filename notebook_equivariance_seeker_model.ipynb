{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cU67irCkAoSz"
   },
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pablocalvo7/Equivariance_Seeker_Model/blob/main/notebook_equivariance_seeker_model.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Machine learning for detection of equivariant finite symmetry groups in dynamical systems**\n",
    "---\n",
    "Pablo Calvo-Barlés,$^{1, 2}$ Sergio G. Rodrigo,$^{1, 3}$ and Luis Martín-Moreno $^{1,2}$\n",
    "\n",
    "---\n",
    "+ $^{1}$ Instituto de Nanociencia y Materiales de Aragón (INMA), CSIC-Universidad de Zaragoza, Zaragoza 50009, Spain\n",
    "+ $^{2}$ Departamento de Física de la Materia Condensada, Universidad de Zaragoza, Zaragoza 50009, Spain\n",
    "+ $^{3}$ Departamento de Física Aplicada, Universidad de Zaragoza, Zaragoza 50009, Spain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7pryPkvt-de"
   },
   "source": [
    "**Abstract**\n",
    "---\n",
    "\n",
    "This Jupyter notebook provides an implementation of the Equivariance Seeker Model (ESM) based on Tensorflow-Keras (it requires Tensorflow version 2.17 or higher). The ESM is designed to find the entire finite symmetry group of a given dynamical system given a dataset of trajectories. We illustrate its functionality with the application to a simple one-dimensional system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPUbrcV0j9gH"
   },
   "source": [
    "**General description**\n",
    "---\n",
    "\n",
    "+ Problem statement: We consider nonlinear dynamical systems described by ordinary differential equations:\n",
    "\\begin{equation}\n",
    "  \\frac{d \\vec{x}}{dt} = \\vec{y}(\\vec{x}),\n",
    "\\end{equation}\n",
    "where $\\vec{x} (t) \\in \\mathbb{R}^n$ denotes the system's state evolving over time $t$, and $\\vec{y} \\left( \\vec{x} (t) \\right) \\in \\mathbb{R}^n$ is the time derivative of the state, provided by the nonlinear vector field $\\vec{y}: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n$ that describes the system's evolution dynamics. The system presents a finite symmetry group of order $K$ given by the $n \\times n$ matrix representation $\\{ \\hat{D}_1 , \\dots , \\hat{D}_K \\}$. This means that the function $\\vec{y} ( \\vec{x} )$ is equivariant under the action of each group transformation, that is,\n",
    "\\begin{equation}\n",
    "\\vec{y}(\\vec{x}) = \\hat{D}_{\\alpha}^{-1} \\vec{y}(\\hat{D}_{\\alpha} \\vec{x}), \\; \\forall \\vec{x} \\in \\mathbb{R}^n, \\; \\forall \\alpha\n",
    "\\end{equation}\n",
    "\n",
    "+ Goal: find all system's symmetry transformations $\\hat{D}_{\\alpha}$ given a set of $N_{\\mathrm{tr}}$ state trajectories $\\{ \\vec{x}^{(r)}(t_s) \\}$ with $N_{\\mathrm{steps}}$ time steps, and their corresponding time derivatives $\\{ \\vec{y}^{(r)}(t_s) \\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvZDxNBvj9gH"
   },
   "source": [
    "## Equivariance Seeker Model (ESM)\n",
    "\n",
    "+ Input: state vector $\\vec{x}$.\n",
    "\n",
    "+ Processing architecture: $M$ parallel branches composed by three blocks: a trainable matrix $\\hat{W}_{\\alpha} \\in \\mathbb{R}^{n \\times n}$, (2) a non-trainable block $\\vec{\\mathcal{Y}}$ which replicates the system's derivative vector field $\\vec{y}$, and (3) the inverse matrix of the first block, $\\hat{W}_{\\alpha}^{-1}$. The trainable weights consist of the set of branch matrices $\\{ \\hat{W}_{\\alpha}\\}$\n",
    "\n",
    "+ Output: $M$ vectors $\\vec{y}_{W}^{(\\alpha)} (\\vec{x}) = \\hat{W}_{\\alpha}^{-1} \\vec{\\mathcal{Y}} \\left( \\hat{W}_{\\alpha} \\vec{x} \\right)$, each one intended to approximate $\\vec{y} (\\vec{x})$.\n",
    "\n",
    "+ The loss function is composed by the sum of two terms. First, the equivariance loss function,\n",
    "\\begin{equation}\n",
    "  \\mathcal{L}_{\\mathrm{equiv}} \\left( \\{ \\hat{W}_{\\alpha} \\} \\right) = \\frac{1}{M} \\sum_{\\alpha = 1}^{M} l_{\\mathrm{equiv}} ( \\hat{W}_{ \\alpha } ),\n",
    "\\end{equation}\n",
    "where\n",
    "\\begin{equation}\n",
    "  l_{\\mathrm{equiv}} \\left( \\hat{W}_{\\alpha} \\right) = \\frac{1}{N} \\sum_{i=1}^{N} || \\vec{y}_i - \\vec{y}^{(\\alpha)}_{W} ( \\vec{x}_i  ) ||^2.\n",
    "\\end{equation}\n",
    "This promotes the branch matrices to become symmetry transformations. Second, the repetition loss function,\n",
    "\\begin{equation}\n",
    "  \\mathcal{L}_{\\mathrm{rep}} \\left( \\{ \\hat{W}_{\\alpha} \\} \\right) = A \\sum_{\\alpha = 1}^{M} \\sum_{\\beta = 1}^{\\alpha - 1} \\exp{ \\left( -\\frac{1}{\\sigma} || \\hat{W}_{\\alpha} - \\hat{W}_{\\beta} ||_{F}^{2} \\right) },\n",
    "\\end{equation}\n",
    "which penalizes that two or more branch matrices converge to the same symmetry transformation.\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ff0qeC8Tj9gH"
   },
   "source": [
    "ESM scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wnD_FgCij9gI"
   },
   "source": [
    "![image](./FIG_ESM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKmf3Ze5j9gI"
   },
   "source": [
    "## Group metric\n",
    "\n",
    "To further demonstrate the symmetry group discovery, we use the group metric,\n",
    "\\begin{equation}\n",
    " d_{\\mathrm{group}} = d_{\\mathrm{closed}} + d_{\\mathrm{inverse}},\n",
    "\\end{equation}\n",
    "which is defined as the sum of two terms: the \"closed metric\",\n",
    "\\begin{equation}\n",
    "  d_{\\mathrm{closed}} = \\frac{1}{M^2} \\sum_{\\alpha = 1}^M \\sum_{\\beta = 1}^M \\min_{\\gamma} d \\left( \\hat{W}_{\\alpha} \\hat{W}_{\\beta} , \\hat{W}_{\\gamma} \\right),\n",
    "\\end{equation}\n",
    "and the \"inverse metric\",\n",
    "\\begin{equation}\n",
    "   d_{\\mathrm{inverse}} = \\frac{1}{M} \\sum_{\\alpha = 1}^M \\min_{\\gamma} d \\left( \\hat{W}_{\\alpha}^{-1} , \\hat{W}_{\\gamma} \\right),\n",
    "\\end{equation}\n",
    "where the element-wise average absolute difference between two arbitrary matrices $\\hat{A}$ and $\\hat{B}$ is defined as\n",
    "\\begin{equation}\n",
    "  d \\left( \\hat{A} , \\hat{B} \\right) = \\frac{1}{n^2} \\sum_{k l} |a_{kl} - b_{kl}|.\n",
    "\\end{equation}\n",
    "This group metric is a non-negative quantity that is equal to zero if and only if the set $\\{ \\hat{W}_{\\alpha} \\}$ forms a group.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mOA8fqfpj9gI"
   },
   "source": [
    "**Python implementation**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sD3mQaex2x8o"
   },
   "source": [
    "## Main imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 284,
     "status": "ok",
     "timestamp": 1736496340839,
     "user": {
      "displayName": "SERGIO GUTIERREZ RODRIGO",
      "userId": "07959720391705098820"
     },
     "user_tz": -60
    },
    "id": "HCuTD9gJ2wcD",
    "outputId": "f556d2a7-d47b-44c5-87e2-25b8faa67472"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ayw_FM1sj9gJ"
   },
   "source": [
    "## Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 257,
     "status": "ok",
     "timestamp": 1736496344092,
     "user": {
      "displayName": "SERGIO GUTIERREZ RODRIGO",
      "userId": "07959720391705098820"
     },
     "user_tz": -60
    },
    "id": "ReoH4Bxpj9gJ"
   },
   "outputs": [],
   "source": [
    "def reset_random_seeds(seed):\n",
    "   os.environ['PYTHONHASHSEED']=str(seed)\n",
    "   tf.random.set_seed(seed)\n",
    "   np.random.seed(seed)\n",
    "   random.seed(seed)\n",
    "\n",
    "def print_history(epoch, epochs, L_equiv, L_rep, A, sigma, eta):\n",
    "    print( \"EPOCH #\",epoch,\"/\",epochs,\n",
    "                \" ; EQUIV LOSS = \",tf.get_static_value(L_equiv),\n",
    "                \" ; REP LOSS = \",tf.get_static_value(L_rep),\n",
    "                \" ; A = \", tf.get_static_value(A),\n",
    "                \" ; sigma = \",tf.get_static_value(sigma),\n",
    "                \" ; eta = \", eta )\n",
    "\n",
    "def print_solutions(Ws,M):\n",
    "\n",
    "    tf.print(\"#######################################\")\n",
    "    tf.print(\"FINAL BRANCH MATRICES:\")\n",
    "    tf.print(\"#######################################\")\n",
    "    for i in range(int(M)):\n",
    "        tf.print(\"W\"+str(i+1)+\" = \")\n",
    "        tf.print(tf.get_static_value(Ws[i,:,:]))\n",
    "        tf.print(\"--------------------------------------\")\n",
    "        tf.print(\"\")\n",
    "\n",
    "    tf.print(\"#######################################\")\n",
    "\n",
    "    pass\n",
    "\n",
    "def y_analytical(x):\n",
    "    dx_dt = -x*x*x\n",
    "    return dx_dt\n",
    "\n",
    "\n",
    "def history_param_exp(epochs, epoch_init, epoch_final, param_init, param_final):\n",
    "    \"\"\"Provides an hyperparameter history with exponential variation as\n",
    "    explained in the paper.\n",
    "\n",
    "    Args:\n",
    "        epochs (int): Total number of hyperparameter values.\n",
    "        epoch_init (int): Starting epoch for the variation interval.\n",
    "        epoch_final (int): Final epoch for the variation interval.\n",
    "        param_init (float): Initial hyperparameter value.\n",
    "        param_final (float): Final hyperparameter value.\n",
    "\n",
    "    Returns:\n",
    "        history_param (ndarray): Array of size (epochs,) containing the\n",
    "            hyperparameter values for each epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    history_epochs = np.array(range(epochs))\n",
    "    rate = np.log(param_init/param_final)/(epoch_final-epoch_init)\n",
    "    history_param = param_init*np.exp(-rate*(history_epochs - epoch_init))\n",
    "    history_param[history_epochs<epoch_init] = param_init\n",
    "    history_param[history_epochs>epoch_final] = param_final\n",
    "\n",
    "    return history_param\n",
    "\n",
    "def calculate_group_metrics(Ws):\n",
    "    \"\"\"Calculates the closed and inverse metrics for a given set of branch\n",
    "        matrices.\n",
    "\n",
    "    Args:\n",
    "        Ws (ndarray): Set of branch matrices.\n",
    "\n",
    "    Returns:\n",
    "        d_closed (float): Closed metric.\n",
    "        d_inverse (float): Inverse metric.\n",
    "    \"\"\"\n",
    "\n",
    "    M = np.shape(Ws)[0]\n",
    "    n = np.shape(Ws)[1]\n",
    "\n",
    "    # Closed metric\n",
    "    Wsa = np.expand_dims(Ws,axis = 0)\n",
    "    Wsb = np.expand_dims(Ws,axis = 1)\n",
    "    Wsa_mul_Wsb = np.matmul(Wsa,Wsb)\n",
    "    Wsa_mul_Wsb_exp = np.expand_dims(Wsa_mul_Wsb,axis=2)\n",
    "    Wsa_mul_Wsb_Wsg = Wsa_mul_Wsb_exp - np.reshape(Ws,(1,1,M,n,n))\n",
    "    d_ab_g = np.sum( np.abs(Wsa_mul_Wsb_Wsg) , axis=(3,4))/(n*n)\n",
    "    d_ab_ming = np.amin(d_ab_g, axis = 2)\n",
    "    d_closed = np.sum( d_ab_ming/(M*M) )\n",
    "\n",
    "    # Inverse metric\n",
    "    Wsa_inv = np.linalg.inv(Ws)\n",
    "    Wsa_inv = np.expand_dims(Wsa_inv, axis = 0)\n",
    "    Wsg = np.expand_dims(Ws,axis = 1)\n",
    "    Wsa_inv_Wsg = Wsa_inv - Wsg\n",
    "    d_a_inv_g = np.sum( np.abs(Wsa_inv_Wsg), axis = (2,3) )/(n*n)\n",
    "    d_a_inv_ming = np.amin(d_a_inv_g, axis = 0)\n",
    "    d_inverse = np.sum(d_a_inv_ming)/M\n",
    "\n",
    "    return d_closed, d_inverse\n",
    "\n",
    "def calculate_history_group_metric(history_Ws):\n",
    "    \"\"\"Calculates the all group metric values for a given history of branch\n",
    "    matrix sets.\n",
    "\n",
    "    Args:\n",
    "        history_Ws (list of 'tf.Tensor' objects): Set of branch matrices.\n",
    "\n",
    "    Returns:\n",
    "        history_d_group (list of floats): Group metric values as function of\n",
    "            the epochs.\n",
    "    \"\"\"\n",
    "\n",
    "    history_d_group = []\n",
    "    for Ws in history_Ws:\n",
    "        Ws_np = Ws.numpy()\n",
    "        d_closed, d_inverse = calculate_group_metrics(Ws_np)\n",
    "        history_d_group.append(d_closed + d_inverse)\n",
    "\n",
    "    return history_d_group\n",
    "\n",
    "def runge_kutta(state_0, F, dt, Nsteps):\n",
    "    \"\"\"Generetes a trayectory of states with the Runge-Kutta method.\n",
    "\n",
    "    Args:\n",
    "        state_0 (ndarray): Initial condition.\n",
    "        F (ndarray): Derivative vector field providing the dynamics.\n",
    "        dt (float): Time step.\n",
    "        Nsteps (int): Number of states of the trajectory.\n",
    "\n",
    "    Returns:\n",
    "        trajectory (ndarray): Contains all points of the time evolution.\n",
    "    \"\"\"\n",
    "\n",
    "    trajectory = [state_0]\n",
    "\n",
    "    state_old = state_0\n",
    "    for i in range(Nsteps):\n",
    "\n",
    "        k1 = dt * F(state_old)\n",
    "        k2 = dt * F(state_old + 0.5*k1)\n",
    "        k3 = dt * F(state_old + 0.5*k2)\n",
    "        k4 = dt * F(state_old + k3)\n",
    "        state_new = state_old + (k1 + 2*k2 + 2*k3 + k4) / 6\n",
    "        trajectory.append(state_new)\n",
    "        state_old = state_new\n",
    "\n",
    "    trajectory = np.array(trajectory)\n",
    "\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PtLi_G_-wDdD"
   },
   "source": [
    "##  ESM class and function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 263,
     "status": "ok",
     "timestamp": 1736496347476,
     "user": {
      "displayName": "SERGIO GUTIERREZ RODRIGO",
      "userId": "07959720391705098820"
     },
     "user_tz": -60
    },
    "id": "NvQ-paBYBR_q"
   },
   "outputs": [],
   "source": [
    "class Branches(tf.keras.layers.Layer):\n",
    "    \"\"\"A 'tf.keras.layers.Layer' subclass representing the branches.\n",
    "\n",
    "    Atributes:\n",
    "        M (int): Number of branches.\n",
    "        Y_block (python function): Reproduces the derivative vector field of\n",
    "            the dynamical system.\n",
    "        kernel_initializer ('tf.keras.initializers' object): Keras initializer\n",
    "            for the branch matrices.\n",
    "        Ws ('tf.Tensor' object): Weights representing the branch matrices.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, M, Y_block,\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.5),\n",
    "            **kwargs):\n",
    "\n",
    "        super(Branches, self).__init__(**kwargs)\n",
    "        self.M = M\n",
    "        self.Y_block = Y_block\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"Instantiates the weights of the layer.\n",
    "\n",
    "        Args:\n",
    "            input_shape (int): Dimension of the input state.\n",
    "        \"\"\"\n",
    "\n",
    "        self.Ws = self.add_weight(name='Ws',\n",
    "                            shape=[self.M, input_shape[-1], input_shape[-1]],\n",
    "                            initializer=self.kernel_initializer,\n",
    "                            dtype=tf.float32,\n",
    "                            trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Computes outputs of all branches given a batch of input states.\n",
    "\n",
    "        Args:\n",
    "            Inputs ('tf.Tensor' object): Batch of input states.\n",
    "\n",
    "        Returns:\n",
    "            ys_W ('tf.Tensor' object): Batch of branch outputs.\n",
    "\n",
    "        \"\"\"\n",
    "        x = tf.cast(inputs,tf.float32)\n",
    "        Ws_x = tf.linalg.matmul(x,self.Ws)\n",
    "        y_Ws_x = self.Y_block(Ws_x)\n",
    "        Ws_inv = tf.linalg.inv(self.Ws)\n",
    "        ys_W = tf.matmul(y_Ws_x,Ws_inv)\n",
    "\n",
    "        return ys_W\n",
    "\n",
    "def Single_Branch_Equivariance_Losses(y, ys_W):\n",
    "    \"\"\"Computes all single-branch equivariance losses for a given input batch.\n",
    "\n",
    "    Args:\n",
    "        y ('tf.Tensor' object): Batch of true derivative vector field\n",
    "            values.\n",
    "        ys_W ('tf.Tensor' object): Batch of ESM predictions.\n",
    "\n",
    "    Returns:\n",
    "        sb_eq_loss ('tf.Tensor' object): All single-branch equivariance losses\n",
    "            for the given input batch.\n",
    "    \"\"\"\n",
    "    sq_error = tf.square( tf.cast(y,tf.float32) - ys_W )\n",
    "    N = tf.shape(y)[0]\n",
    "    sb_eq_loss = tf.reduce_sum(sq_error, axis = (1,2)) / tf.cast(N, tf.float32)\n",
    "\n",
    "    return sb_eq_loss\n",
    "\n",
    "\n",
    "class Equivariance_Loss(tf.keras.losses.Loss):\n",
    "    \"\"\"A 'keras.losses.Loss' subclass representing the equivariance loss\n",
    "        function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, reduction = 'sum',**kwargs):\n",
    "        super().__init__(reduction = reduction, **kwargs)\n",
    "\n",
    "    def call(self, y, ys_W):\n",
    "        \"\"\"Computes the equivariance loss function for a given input batch.\n",
    "\n",
    "        Args:\n",
    "            y ('tf.Tensor' object): Batch of true derivative vector field\n",
    "                values.\n",
    "            ys_W ('tf.Tensor' object): Batch of ESM predictions.\n",
    "\n",
    "        Returns:\n",
    "            eq_loss ('tf.Tensor' object): Equivariance loss function for the\n",
    "                given input batch.\n",
    "        \"\"\"\n",
    "        sq_error = tf.square( tf.cast(y,tf.float32) - ys_W )\n",
    "        N = tf.shape(y)[0]\n",
    "        M = tf.shape(ys_W)[0]\n",
    "        eq_loss = sq_error / tf.cast(N*M, tf.float32)\n",
    "\n",
    "        return eq_loss\n",
    "\n",
    "class Equivariance_Metric(tf.keras.metrics.Metric):\n",
    "    \"\"\"A 'keras.metrics.Metric' subclass representing the equivariance loss\n",
    "    function.\n",
    "\n",
    "    Atributes:\n",
    "        equiv_loss ('Equivarance_Loss' object): Equivariance loss function.\n",
    "        total ('tf.Tensor' object): Accumulates the equivariance losses from\n",
    "            each batch.\n",
    "        count: ('tf.Tensor' object): Sums 1 every time a batch is evaluated.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.equiv_loss = Equivariance_Loss()\n",
    "        self.total = self.add_weight(shape=(1,), initializer=\"zeros\",name=\"total\")\n",
    "        self.count = self.add_weight(shape=(1,), initializer=\"zeros\",name=\"count\")\n",
    "\n",
    "\n",
    "    def update_state(self, y, ys_W):\n",
    "        \"\"\"Update the equivariance metric every time a batch is evaluated.\n",
    "\n",
    "        Args:\n",
    "            y ('tf.Tensor' object): Batch of true derivative vector field\n",
    "                values.\n",
    "            ys_W ('tf.Tensor' object): Batch of ESM predictions.\n",
    "\n",
    "        \"\"\"\n",
    "        equiv_metric = self.equiv_loss(y, ys_W)\n",
    "        self.total.assign_add(equiv_metric)\n",
    "        self.count.assign_add(tf.cast(1.0, tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.total/self.count\n",
    "\n",
    "def pairwise_matrix_distances(Ws):\n",
    "    \"\"\"Computes euclidean distances between all possible matrix pairs of a\n",
    "    given set.\n",
    "\n",
    "    Args:\n",
    "        Ws ('tf.Tensor' object): Set of branch matrices.\n",
    "\n",
    "    Returns:\n",
    "        distances_matrix ('tf.Tensor' object): Matrix containing the distances\n",
    "            between each pair of matrices.\n",
    "    \"\"\"\n",
    "\n",
    "    Wsa = tf.expand_dims(Ws,axis = 0)\n",
    "    Wsb = tf.expand_dims(Ws,axis = 1)\n",
    "\n",
    "    Wsa_Wsb2 = (Wsa - Wsb)**2\n",
    "    distances_matrix = tf.reduce_sum(Wsa_Wsb2, axis=(2,3))\n",
    "\n",
    "    return distances_matrix\n",
    "\n",
    "\n",
    "class Repetiton_Loss():\n",
    "    \"\"\"A class representing the repetition loss function.\n",
    "\n",
    "    Atributes:\n",
    "        sigma ('tf.Tensor' object): Gaussian width.\n",
    "        A: ('tf.Tensor' object): Gaussian amplitude.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 sigma = tf.constant(0.0, dtype = tf.float32),\n",
    "                 A = tf.constant(0.0, dtype = tf.float32)):\n",
    "\n",
    "        self.sigma = sigma\n",
    "        self.A = A\n",
    "\n",
    "    def __call__(self, Ws):\n",
    "        \"\"\"Computes the repetition loss function from a set of branch matrices.\n",
    "\n",
    "        Args:\n",
    "            Ws ('tf.Tensor' object): Branch matrices.\n",
    "\n",
    "        Returns:\n",
    "            L_rep ('tf.Tensor' object): Repetition loss.\n",
    "        \"\"\"\n",
    "\n",
    "        d_matrix = pairwise_matrix_distances(Ws)\n",
    "\n",
    "        # We substract \"terms_1_diagonal\" to remove the diagonal elements '1'\n",
    "        # of the matrix tf.math.exp(-lambd*d2_tensor) (e^0 = 1)\n",
    "        terms_1_diagonal = tf.cast(tf.shape(Ws)[0],tf.float32)\n",
    "\n",
    "        # The factor 0.5 ensures the sum for elements (a,b) such that a<b\n",
    "        L_rep = self.A*0.5*(tf.reduce_sum(tf.math.exp(-d_matrix/self.sigma))-\\\n",
    "            terms_1_diagonal )\n",
    "\n",
    "        return L_rep\n",
    "\n",
    "class ESM(tf.keras.Model):\n",
    "    \"\"\"A 'tf.keras.Model' subclass representing the Equivariance Seeker Model.\n",
    "\n",
    "    Atributes:\n",
    "        Branches ('Branches' object): The layer representing the branches.\n",
    "        M (int) object: Number of branches. This is taken from 'Branches'\n",
    "            atribute.\n",
    "        Y_block (python function): Reproduces the derivative vector field of\n",
    "            the dynamical system. This is taken from 'Branches' atribute.\n",
    "        equivariance_loss ('Equivariance_Loss' object): Loss to promote\n",
    "            equivariance condition.\n",
    "        equivariance_metric ('Equivariance_Metric' object): Metric to measure\n",
    "            how well the equivariance condition is fulfill.\n",
    "        repetiton_loss ('Repetition_Loss' object): Loss to penalize branch\n",
    "            matrix repetition.\n",
    "        optimizer ('tf.keras.optimizers' object): Optimizer for ESM training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, Branches,\n",
    "                 equivariance_loss = Equivariance_Loss(),\n",
    "                 equivariance_metric = Equivariance_Metric(),\n",
    "                 repetition_loss = Repetiton_Loss(),\n",
    "                 optimizer = tf.keras.optimizers.SGD(),\n",
    "                 **kwargs):\n",
    "\n",
    "        super(ESM, self).__init__(**kwargs)\n",
    "        self.Branches = Branches\n",
    "        self.M = Branches.M\n",
    "        self.Y_block = Branches.Y_block\n",
    "        self.equivariance_loss = equivariance_loss\n",
    "        self.equivariance_metric = equivariance_metric\n",
    "        self.repetition_loss = repetition_loss\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        ys_W = self.Branches(inputs)\n",
    "        return ys_W\n",
    "\n",
    "    def train_evaluation_step(self, x_batch, y_batch, sigma, A):\n",
    "        \"\"\"Performs an ESM training step given a batch of data.\n",
    "\n",
    "        This method computes the training losses and their gradients. Then, it\n",
    "        updates the ESM branch matrices by applying those gradients and finally\n",
    "        update the metrics.\n",
    "\n",
    "        Args:\n",
    "            x_batch ('tf.Tensor' object): Batch of input states.\n",
    "            y_batch ('tf.Tensor' object): Batch of output states.\n",
    "            sigma (scalar 'tf.Tensor' object): Gaussian width.\n",
    "            A (scalar 'tf.Tensor' object): Gaussian amplitude.\n",
    "\n",
    "        Returns:\n",
    "            equiv_metric (scalar 'tf.Tensor' object): Updated equivariance\n",
    "                metric value.\n",
    "            rep_loss (scalar 'tf.Tensor' object): Updated repetition loss value.\n",
    "            Ws ('tf.Tensor' objec): Updated branch matrices.\n",
    "        \"\"\"\n",
    "\n",
    "        # Set repetiton loss hyperparameters\n",
    "        self.repetition_loss.sigma = sigma\n",
    "        self.repetition_loss.A = A\n",
    "\n",
    "        # Gradient tape\n",
    "        with tf.GradientTape() as tape:\n",
    "            ys_W_batch = self(x_batch, training=True)\n",
    "            equiv_loss = self.equivariance_loss(y_batch, ys_W_batch)\n",
    "            rep_loss = self.repetition_loss(self.trainable_variables[0])\n",
    "            loss_value = equiv_loss + rep_loss\n",
    "\n",
    "        # Apply gradients\n",
    "        grads = tape.gradient(loss_value, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "\n",
    "        # Update equivariance metric\n",
    "        equiv_metric = self.equivariance_metric(y_batch, ys_W_batch)\n",
    "\n",
    "        return equiv_metric, rep_loss, self.Branches.Ws,self.trainable_variables,equiv_loss,rep_loss\n",
    "\n",
    "    def remove_branches_local_minima(self, x, y, limits_global_minima):\n",
    "        \"\"\"Removes branches from the ESM that have not converged to global\n",
    "        minima.\n",
    "\n",
    "        Args:\n",
    "            x (ndarray): Input traninig states.\n",
    "            y (ndarray): Output training state derivatives.\n",
    "            limits_global_minima (tuple): Minimum and maximum values of the\n",
    "                single-branch equivariance loss used to determine whether a\n",
    "                branch matrix has converged to a global minimum.\n",
    "\n",
    "        Returns:\n",
    "            sb_eq_loss ('tf.Tensor' object): All single-branch equivariance\n",
    "            losses before the branch removal.\n",
    "        \"\"\"\n",
    "\n",
    "        ys_W = self(x)\n",
    "        sb_eq_losses = Single_Branch_Equivariance_Losses(y, ys_W)\n",
    "\n",
    "        idxs_global_minima = []\n",
    "        for k,L in enumerate(sb_eq_losses):\n",
    "            if (L > limits_global_minima[0]) and (L < limits_global_minima[1]):\n",
    "                idxs_global_minima.append(k)\n",
    "\n",
    "        if( len(idxs_global_minima) > 0 ):\n",
    "            Ws = self.Branches.Ws\n",
    "            Ws_new = tf.gather(Ws,indices=idxs_global_minima,axis=0)\n",
    "            M_new = len(idxs_global_minima)\n",
    "            self.Branches.M = M_new\n",
    "            self.M = M_new\n",
    "            self.Branches.Ws = Ws_new\n",
    "        \n",
    "        return sb_eq_losses\n",
    "\n",
    "\n",
    "    def sort_distances_matrix(self, d_matrix):\n",
    "        \"\"\"Sorts the pairwise distances matrix to group similar matrices\n",
    "        together.\n",
    "\n",
    "        Args:\n",
    "            d_matrix ('tf.Tensor' object): Pairwise distances matrix.\n",
    "\n",
    "        Returns:\n",
    "            d_matrix_np (ndarray): Sorted pairswise distances matrix.\n",
    "            idxs_sort (list): List of sorted branch indices.\n",
    "        \"\"\"\n",
    "\n",
    "        d_matrix_np = d_matrix.numpy()\n",
    "        idxs_sort = [i for i in range(self.M)]\n",
    "\n",
    "        for a in range(self.M-1):\n",
    "            index_min_a = np.argmin(d_matrix_np[a+1:,a]) + a + 1\n",
    "            d_matrix_np[[a+1,index_min_a],:] = d_matrix_np[[index_min_a,a+1],:]\n",
    "            d_matrix_np[:,[a+1,index_min_a]] = d_matrix_np[:,[index_min_a,a+1]]\n",
    "            idxs_sort[a+1],idxs_sort[index_min_a] =\\\n",
    "                                        idxs_sort[index_min_a],idxs_sort[a+1]\n",
    "\n",
    "        return d_matrix_np, idxs_sort\n",
    "\n",
    "    def remove_redundant_branches(self, threshold_distance):\n",
    "        \"\"\"Removes redundant branches from the ESM.\n",
    "\n",
    "        Args:\n",
    "            threshold_distance (float): Maximum pairwise distance for\n",
    "                considering two matrices as identical.\n",
    "\n",
    "        Returns:\n",
    "            d_matrix_sorted (ndarray): Sorted pairswise distances matrix.\n",
    "        \"\"\"\n",
    "\n",
    "        Ws = self.Branches.Ws\n",
    "        M = self.M\n",
    "\n",
    "        # Calculate pairwise distances matrix\n",
    "        d_matrix = pairwise_matrix_distances(Ws)\n",
    "\n",
    "        # Sort d_matrix and take idxs sorted\n",
    "        d_matrix_sorted, idxs_sort = self.sort_distances_matrix(d_matrix)\n",
    "\n",
    "        # Sort Ws according to d_matrix\n",
    "        Ws_sort = tf.gather(Ws,indices=idxs_sort,axis=0)\n",
    "\n",
    "        # Take the first idx of each block in d_matrix\n",
    "        idxs_new = [0]\n",
    "        a = 0\n",
    "        while(a<M):\n",
    "            b = a\n",
    "            distance = d_matrix_sorted[a,b]\n",
    "            while(distance < threshold_distance and b<M):\n",
    "                b = b + 1\n",
    "                if(b<M):\n",
    "                    distance = d_matrix_sorted[a,b]\n",
    "\n",
    "            if(b<M):\n",
    "                idxs_new.append(b)\n",
    "            a = b\n",
    "\n",
    "        # Remove redundant matrices\n",
    "        M_new = len(idxs_new)\n",
    "        Ws_new = tf.gather(Ws_sort,indices=idxs_new,axis=0)\n",
    "        self.Branches.M = M_new\n",
    "        self.M = M_new\n",
    "        self.Branches.Ws=Ws_new\n",
    "\n",
    "        return d_matrix_sorted\n",
    "\n",
    "    def training_loop(self,\n",
    "                      x,\n",
    "                      y,\n",
    "                      mbs,\n",
    "                      epochs,\n",
    "                      epoch_remove_local_minima,\n",
    "                      limits_global_minima,\n",
    "                      epoch_remove_redundant,\n",
    "                      threshold_distance,\n",
    "                      history_sigma,\n",
    "                      history_A,\n",
    "                      history_eta,\n",
    "                      seed = 655720):\n",
    "        \"\"\"This method performs the ESM training.\n",
    "\n",
    "        Args:\n",
    "            x (ndarray): Input training data set.\n",
    "            y (ndarray): Output training data set.\n",
    "            mbs (int): Mini-batch size.\n",
    "            epochs (int): Total number of epochs.\n",
    "            history_sigma (ndarray): Array of size (epochs,) containing the\n",
    "                gaussian width values at each training epoch.\n",
    "            history_A (ndarray): Array of size (epochs,) containing the\n",
    "                gaussian amplitude values at each training epoch.\n",
    "            history_eta (ndarray): Array of size (epochs,) containing the\n",
    "                learning rate values at each training epoch.\n",
    "            seed (int): Seed for random weight initialization.\n",
    "\n",
    "        Returns:\n",
    "            history_Lequiv (list): List containing equivariance loss values for\n",
    "                each training epoch.\n",
    "            history_Lrep (list): List containing repetition loss values for\n",
    "                each training epoch.\n",
    "            history_Ws (list): List containing the branch matrices for each\n",
    "                training epoch.\n",
    "        \"\"\"\n",
    "\n",
    "        # Data set\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "        train_dataset = train_dataset.shuffle(buffer_size=1024).batch(mbs)\n",
    "\n",
    "        # Reset seed\n",
    "        reset_random_seeds(seed)\n",
    "\n",
    "        # Initialize history lists\n",
    "        history_Lequiv = []\n",
    "        history_Lrep = []\n",
    "        history_Ws = []\n",
    "\n",
    "        train_ev_graph_func = tf.function(self.train_evaluation_step)\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # Hyperparameter tuning\n",
    "            sigma = tf.cast( history_sigma[epoch] , dtype=tf.float32 )\n",
    "            A = tf.cast( history_A[epoch] , dtype=tf.float32 )\n",
    "            eta = history_eta[epoch]\n",
    "            self.optimizer.learning_rate = eta\n",
    "\n",
    "            # First branch removal process\n",
    "            if(epoch == epoch_remove_local_minima):\n",
    "                sb_eq_losses = self.remove_branches_local_minima(x,\n",
    "                                                        y,\n",
    "                                                        limits_global_minima)\n",
    "                train_ev_graph_func = tf.function(self.train_evaluation_step)\n",
    "\n",
    "            # Second branch removal process\n",
    "            if(epoch == epoch_remove_redundant):\n",
    "                d_matrix = self.remove_redundant_branches(threshold_distance)\n",
    "                train_ev_graph_func = tf.function(self.train_evaluation_step)\n",
    "\n",
    "            # Iterate over the batches of the data set\n",
    "            for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
    "\n",
    "                equiv_metric, rep_loss, Ws, train_var,equiv_loss,rep_loss= train_ev_graph_func(x_batch,\n",
    "                                                                 y_batch,\n",
    "                                                                 sigma,\n",
    "                                                                 A)\n",
    "\n",
    "            # Save metrics and weights\n",
    "            history_Lequiv.append( equiv_metric )\n",
    "            history_Lrep.append( rep_loss )\n",
    "            history_Ws.append( Ws )\n",
    "\n",
    "            # Reset equivariance metric\n",
    "            self.equivariance_metric.reset_state()\n",
    "\n",
    "            # Print history\n",
    "            if(epoch%10==0):\n",
    "                print_history(epoch, epochs,\n",
    "                              equiv_metric, rep_loss,\n",
    "                              A, sigma, eta)\n",
    "\n",
    "        # Print solutions\n",
    "        print_solutions(Ws,self.M)\n",
    "\n",
    "        return history_Lequiv, history_Lrep, history_Ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_i6a8C1xj9gK"
   },
   "source": [
    "## Application to 1D example\n",
    "\n",
    "We apply the method described in the paper to a simple dynamical system whose equation is given by\n",
    "\\begin{equation}\n",
    "  \\frac{dx}{dt} = -x^3.\n",
    "\\end{equation}\n",
    "The system presents a symmetry group of order $K = 2$, whose elements are $D_1 = 1$ and $D_2 = -1$. In this demonstration, we use that the analytical expression $y(x) = -x^3$ in the ESM. However, as explained in the main text, the expression can be substituted by a pretrained oracle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-9ic6G7j9gK"
   },
   "source": [
    "Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 307,
     "status": "ok",
     "timestamp": 1736496351059,
     "user": {
      "displayName": "SERGIO GUTIERREZ RODRIGO",
      "userId": "07959720391705098820"
     },
     "user_tz": -60
    },
    "id": "ZyTtXQ9Dj9gK"
   },
   "outputs": [],
   "source": [
    "# Trajectory settings settings\n",
    "dt = 1e-3\n",
    "Nsteps = 100\n",
    "Ntrajectories = 10\n",
    "\n",
    "# Domain of initial conditions\n",
    "x_min = -2.5\n",
    "x_max = 2.5\n",
    "\n",
    "x=[]\n",
    "\n",
    "for i in range(Ntrajectories):\n",
    "\n",
    "    # Initial condition\n",
    "    state_0 = np.random.uniform(x_min, x_max)\n",
    "\n",
    "    # Calculate trajectory with Runge-Kutta method\n",
    "    trajectory = runge_kutta(state_0, y_analytical, dt, Nsteps)\n",
    "\n",
    "    x.append(trajectory)\n",
    "\n",
    "x = np.concatenate(x,axis = 0)\n",
    "y = y_analytical(x)\n",
    "\n",
    "\n",
    "# Random permutation of samples\n",
    "Nsamples = np.shape(x)[0]\n",
    "list_indices = [i for i in range(Nsamples)]\n",
    "shuffle_list = np.random.permutation(list_indices)\n",
    "x = x[shuffle_list]\n",
    "y = y[shuffle_list]\n",
    "\n",
    "x = np.reshape(x, (Nsamples, 1))\n",
    "y = np.reshape(y, (Nsamples, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjBQupLaj9gL"
   },
   "source": [
    "Build and run the ESM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 75774,
     "status": "ok",
     "timestamp": 1736496429787,
     "user": {
      "displayName": "SERGIO GUTIERREZ RODRIGO",
      "userId": "07959720391705098820"
     },
     "user_tz": -60
    },
    "id": "b20FbOlCj9gL",
    "outputId": "8a4f3cd2-70b5-4cea-97c2-6aee41089dcb"
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "M = 4\n",
    "branches = Branches(M = M, Y_block = y_analytical)\n",
    "model = ESM(Branches = branches, optimizer = tf.keras.optimizers.RMSprop())\n",
    "\n",
    "# Hyperparameters\n",
    "sigmas = (1e1, 1e-2)\n",
    "As = (1e3, 1e-8)\n",
    "etas = (1e-3, 1e-6)\n",
    "epochs = 1900\n",
    "interval_sigma = (400, 800)\n",
    "interval_A = (800, 1200)\n",
    "interval_eta = (1200, 1600)\n",
    "epoch_remove_local_minima = 1700\n",
    "epoch_remove_redundant = 1800\n",
    "limits_global_minima = (0.0, 1e-5)\n",
    "threshold_distance = 1e-2\n",
    "minib_size = 64\n",
    "\n",
    "# Width, amplitude and learning rate history\n",
    "history_sigma = history_param_exp(epochs,\n",
    "                    interval_sigma[0], interval_sigma[1],\n",
    "                    sigmas[0], sigmas[1])\n",
    "history_A = history_param_exp(epochs,\n",
    "                    interval_A[0], interval_A[1],\n",
    "                    As[0], As[1])\n",
    "history_eta = history_param_exp(epochs,\n",
    "                    interval_eta[0], interval_eta[1],\n",
    "                    etas[0], etas[1])\n",
    "\n",
    "# Training\n",
    "history_Lequiv, history_Lrep, history_Ws, = model.training_loop(x,\n",
    "                                                    y,\n",
    "                                                    minib_size,\n",
    "                                                    epochs,\n",
    "                                                    epoch_remove_local_minima,\n",
    "                                                    limits_global_minima,\n",
    "                                                    epoch_remove_redundant,\n",
    "                                                    threshold_distance,\n",
    "                                                    history_sigma,\n",
    "                                                    history_A,\n",
    "                                                    history_eta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KqPzA5--j9gL"
   },
   "source": [
    "Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 254
    },
    "executionInfo": {
     "elapsed": 1646,
     "status": "ok",
     "timestamp": 1736496505313,
     "user": {
      "displayName": "SERGIO GUTIERREZ RODRIGO",
      "userId": "07959720391705098820"
     },
     "user_tz": -60
    },
    "id": "vU_Une5kj9gL",
    "outputId": "d9f94dff-5b0c-479b-fd7e-d9d2163efeda"
   },
   "outputs": [],
   "source": [
    "# Calculate group metric\n",
    "history_dgroup = calculate_history_group_metric(history_Ws)\n",
    "\n",
    "# Plot equivariance loss, repetition loss and group metric\n",
    "fig = plt.figure(constrained_layout=True, figsize=(10,8))\n",
    "\n",
    "gs = fig.add_gridspec(3,1)\n",
    "ax1 = fig.add_subplot(gs[0,0])\n",
    "ax2 = fig.add_subplot(gs[1,0])\n",
    "ax3 = fig.add_subplot(gs[2,0])\n",
    "\n",
    "ax1.set_ylabel(r'$\\mathcal{L}_{\\mathrm{equiv}}$',fontsize=20)\n",
    "ax2.set_ylabel(r'$\\mathcal{L}_{\\mathrm{rep}}$',fontsize=20)\n",
    "ax3.set_ylabel(r'$d_{\\mathrm{group}}$',fontsize=20)\n",
    "ax3.set_xlabel('Epochs',fontsize=20)\n",
    "\n",
    "ax1.plot(range(epochs),history_Lequiv, linewidth=4, c='k')\n",
    "ax2.plot(range(epochs),history_Lrep, linewidth=4, c='k')\n",
    "ax3.plot(range(epochs),history_dgroup, linewidth=4, c='k')\n",
    "\n",
    "ax1.set_yscale('log')\n",
    "ax2.set_yscale('log')\n",
    "ax3.set_yscale('log')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
